{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4bc9e7",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "Parsed JSON Documents\n",
    "    ↓\n",
    "Load with TextChunker\n",
    "    ↓\n",
    "Create semantic chunks\n",
    "    ↓\n",
    "Embed with EmbeddingProvider\n",
    "    ↓\n",
    "Store in PostgreSQL\n",
    "    ↓\n",
    "Vector similarity indexed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fcf85",
   "metadata": {},
   "source": [
    "## Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadf430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "# Import modules from previous notebooks\n",
    "# Assume they're available in the Python environment\n",
    "\n",
    "# Minimal logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Main ETL pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"../data\",\n",
    "        embedder_provider: str = \"sentence-transformers\",\n",
    "        embedder_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        db_host: str = None,\n",
    "        db_port: int = None,\n",
    "        db_name: str = None,\n",
    "        db_user: str = None,\n",
    "        db_password: str = None,\n",
    "        max_chunk_tokens: int = 512,\n",
    "        chunk_overlap_tokens: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize pipeline.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Base data directory path\n",
    "            embedder_provider: \"openai\" or \"sentence-transformers\"\n",
    "            embedder_model: Model name for embedder\n",
    "            db_host: Database host\n",
    "            db_port: Database port\n",
    "            db_name: Database name\n",
    "            db_user: Database user\n",
    "            db_password: Database password\n",
    "            max_chunk_tokens: Maximum tokens per chunk\n",
    "            chunk_overlap_tokens: Token overlap between chunks\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir).resolve()\n",
    "        self.processed_dir = self.data_dir / \"processed\"\n",
    "        \n",
    "        # Initialize components\n",
    "        # from chunker import TextChunker\n",
    "        # from embedder import create_embedder\n",
    "        # from db import DatabaseConnector\n",
    "        \n",
    "        # self.chunker = TextChunker(\n",
    "        #     max_chunk_tokens=max_chunk_tokens,\n",
    "        #     overlap_tokens=chunk_overlap_tokens\n",
    "        # )\n",
    "        # \n",
    "        # self.embedder = create_embedder(\n",
    "        #     provider=embedder_provider,\n",
    "        #     model=embedder_model if embedder_provider == \"sentence-transformers\" else None,\n",
    "        #     api_key=None if embedder_provider != \"openai\" else None\n",
    "        # )\n",
    "        # \n",
    "        # self.db = DatabaseConnector(\n",
    "        #     host=db_host,\n",
    "        #     port=db_port,\n",
    "        #     database=db_name,\n",
    "        #     user=db_user,\n",
    "        #     password=db_password\n",
    "        # )\n",
    "        \n",
    "        logger.info(f\"Pipeline initialized with data dir: {self.data_dir}\")\n",
    "    \n",
    "    def load_parsed_documents(self) -> List[dict]:\n",
    "        \"\"\"Load all parsed JSON documents.\"\"\"\n",
    "        if not self.processed_dir.exists():\n",
    "            raise FileNotFoundError(f\"Processed data directory not found: {self.processed_dir}\")\n",
    "        \n",
    "        documents = []\n",
    "        json_files = list(self.processed_dir.glob(\"*.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            raise ValueError(f\"No JSON files found in {self.processed_dir}\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                doc = json.load(f)\n",
    "                documents.append(doc)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(documents)} parsed documents\")\n",
    "        return documents\n",
    "    \n",
    "    def process_documents(self, documents: List[dict], skip_components: List[str] = None):\n",
    "        \"\"\"\n",
    "        Process documents through full pipeline.\n",
    "        \n",
    "        Args:\n",
    "            documents: Parsed documents from generate_ast.ts\n",
    "            skip_components: Component names to skip\n",
    "        \"\"\"\n",
    "        skip_components = skip_components or []\n",
    "        total_chunks_stored = 0\n",
    "        \n",
    "        for doc in documents:\n",
    "            component = doc.get(\"name\", \"unknown\")\n",
    "            \n",
    "            if component in skip_components:\n",
    "                logger.info(f\"Skipping {component} (in skip list)\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Processing {component}\")\n",
    "            \n",
    "            # Clear previous chunks for this component\n",
    "            # deleted = self.db.delete_component_chunks(component)\n",
    "            # if deleted > 0:\n",
    "            #     logger.info(f\"Deleted {deleted} previous chunks for {component}\")\n",
    "            \n",
    "            # Chunk the document\n",
    "            # chunks = self.chunker.chunk_documents([doc])\n",
    "            # logger.info(f\"Created {len(chunks)} chunks for {component}\")\n",
    "            \n",
    "            # Embed and store each chunk\n",
    "            # for chunk in chunks:\n",
    "            #     try:\n",
    "            #         embedding = self.embedder.embed_single(chunk.content)\n",
    "            #         chunk_id = self.db.store_chunk(\n",
    "            #             component=chunk.component,\n",
    "            #             section_title=chunk.section_title,\n",
    "            #             section_path=chunk.section_path,\n",
    "            #             content=chunk.content,\n",
    "            #             embedding=embedding,\n",
    "            #             demo_files=chunk.demo_files,\n",
    "            #             images=chunk.images,\n",
    "            #             token_count=chunk.token_count\n",
    "            #         )\n",
    "            #         total_chunks_stored += 1\n",
    "            #     except Exception as e:\n",
    "            #         logger.error(\n",
    "            #             f\"Error storing chunk for {component} \"\n",
    "            #             f\"(section: {chunk.section_title}): {e}\"\n",
    "            #         )\n",
    "        \n",
    "        logger.info(f\"Stored {total_chunks_stored} chunks total\")\n",
    "        return total_chunks_stored\n",
    "    \n",
    "    def run(self, skip_components: List[str] = None):\n",
    "        \"\"\"\n",
    "        Run complete pipeline.\n",
    "        \n",
    "        Args:\n",
    "            skip_components: Component names to skip\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting ETL pipeline\")\n",
    "            \n",
    "            # Connect to database\n",
    "            # self.db.connect()\n",
    "            \n",
    "            # Create tables\n",
    "            # logger.info(\"Creating database tables\")\n",
    "            # self.db.create_tables()\n",
    "            \n",
    "            # Load parsed documents\n",
    "            documents = self.load_parsed_documents()\n",
    "            \n",
    "            # Process all documents\n",
    "            total = self.process_documents(documents, skip_components)\n",
    "            \n",
    "            # Log statistics\n",
    "            # stats = self.db.get_stats()\n",
    "            # logger.info(\"Pipeline complete\")\n",
    "            # logger.info(f\"Total chunks in database: {stats['total_chunks']}\")\n",
    "            # logger.info(f\"Average chunk size: {stats['avg_token_count']:.0f} tokens\")\n",
    "            \n",
    "            # return stats\n",
    "            \n",
    "            return {\"chunks_processed\": total}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "            raise\n",
    "        # finally:\n",
    "        #     self.db.disconnect()\n",
    "\n",
    "\n",
    "print(\"Pipeline module loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239906f",
   "metadata": {},
   "source": [
    "## Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ee9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "# pipeline = Pipeline(\n",
    "#     data_dir=\"../data\",\n",
    "#     embedder_provider=\"sentence-transformers\",\n",
    "#     embedder_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "\n",
    "# # Run pipeline with specific components\n",
    "# stats = pipeline.run(skip_components=[\"Button\", \"Checkbox\"])\n",
    "# print(f\"Pipeline results: {stats}\")\n",
    "\n",
    "print(\"Pipeline ready to execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b478526",
   "metadata": {},
   "source": [
    "## CLI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c661bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def create_cli_parser():\n",
    "    \"\"\"Create command-line argument parser.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Run Marigold RAG ETL pipeline\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--embedder\",\n",
    "        choices=[\"openai\", \"sentence-transformers\"],\n",
    "        default=\"sentence-transformers\",\n",
    "        help=\"Embedding provider\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        default=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        help=\"Model name for embedder\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        default=\"../data\",\n",
    "        help=\"Base data directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--skip\",\n",
    "        nargs=\"+\",\n",
    "        default=[],\n",
    "        help=\"Component names to skip\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunk-size\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"Maximum tokens per chunk\"\n",
    "    )\n",
    "    \n",
    "    return parser\n",
    "\n",
    "# Example usage\n",
    "parser = create_cli_parser()\n",
    "print(\"CLI parser created with following options:\")\n",
    "parser.print_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaceb9f",
   "metadata": {},
   "source": [
    "## Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d588d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration summary\n",
    "config_summary = {\n",
    "    \"Input\": \"Parsed JSON from TypeScript parser (etl/data/processed/)\",\n",
    "    \"Chunking\": \"Semantic text chunking with 512 token limit, 50 token overlap\",\n",
    "    \"Embedding\": \"Sentence-transformers (all-MiniLM-L6-v2) or OpenAI API\",\n",
    "    \"Storage\": \"PostgreSQL with pgvector extension for vector indexing\",\n",
    "    \"Output\": \"Vector-indexed chunks with metadata in database\"\n",
    "}\n",
    "\n",
    "for stage, description in config_summary.items():\n",
    "    print(f\"{stage:12} : {description}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
