{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c21dfff",
   "metadata": {},
   "source": [
    "# Semantic Chunker\n",
    "\n",
    "Split parsed MDX documents into semantic chunks based on document structure.\n",
    "\n",
    "**Features:**\n",
    "- Semantic chunking by document sections\n",
    "- Hierarchical structure with parent-child relationships\n",
    "- Demo code and image references\n",
    "- Token statistics for each section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1c6aa",
   "metadata": {},
   "source": [
    "## 1. Data Structures & Chunker Class\n",
    "\n",
    "Defines the `Chunk` data structure and the `TextChunker` class for semantic chunking based on document sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf313068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TextChunker class loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a semantic chunk of content.\"\"\"\n",
    "    component: str\n",
    "    section_title: Optional[str]\n",
    "    section_path: Optional[str]\n",
    "    content: str\n",
    "    demo_files: List[str]\n",
    "    images: List[str]\n",
    "    token_count: int\n",
    "\n",
    "\n",
    "class TextChunker:\n",
    "    \"\"\"Semantic text chunking based on document sections.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_tokens: int = 500, overlap_tokens: int = 50):\n",
    "        \"\"\"Initialize chunker with target token size and overlap.\"\"\"\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(text: str) -> int:\n",
    "        \"\"\"Estimate tokens: approximately 1 token per 4 characters.\"\"\"\n",
    "        return max(1, len(text) // 4)\n",
    "    \n",
    "    def _process_sections(\n",
    "        self,\n",
    "        component: str,\n",
    "        sections: list,\n",
    "        chunks: List[Chunk],\n",
    "        parent_path: str = \"\"\n",
    "    ):\n",
    "        \"\"\"Recursively process sections and extract content.\"\"\"\n",
    "        for section in sections:\n",
    "            if isinstance(section, str):\n",
    "                continue\n",
    "            \n",
    "            section_title = section.get(\"title\", \"\")\n",
    "            section_path = f\"{parent_path} > {section_title}\".strip(\"> \")\n",
    "            \n",
    "            # Gather content\n",
    "            content_parts = []\n",
    "            demo_files = []\n",
    "            images = []\n",
    "            \n",
    "            # Add main content\n",
    "            if content := section.get(\"content\"):\n",
    "                if isinstance(content, str):\n",
    "                    content_parts.append(content)\n",
    "                elif isinstance(content, list):\n",
    "                    for item in content:\n",
    "                        if isinstance(item, str):\n",
    "                            content_parts.append(item)\n",
    "                        elif isinstance(item, dict):\n",
    "                            if item.get(\"type\") == \"paragraph\":\n",
    "                                content_parts.append(item.get(\"text\", \"\"))\n",
    "                            elif item.get(\"type\") == \"demo\":\n",
    "                                if demo_file := item.get(\"file\"):\n",
    "                                    demo_files.append(demo_file)\n",
    "                            elif item.get(\"type\") == \"image\":\n",
    "                                if image_path := item.get(\"src\"):\n",
    "                                    images.append(image_path)\n",
    "            \n",
    "            # Add demo file references\n",
    "            if demos := section.get(\"demos\"):\n",
    "                if isinstance(demos, list):\n",
    "                    for demo in demos:\n",
    "                        if isinstance(demo, str):\n",
    "                            demo_files.append(demo)\n",
    "                        elif isinstance(demo, dict) and (file := demo.get(\"file\")):\n",
    "                            demo_files.append(file)\n",
    "            \n",
    "            # Add image references\n",
    "            if img_list := section.get(\"images\"):\n",
    "                if isinstance(img_list, list):\n",
    "                    for img in img_list:\n",
    "                        if isinstance(img, dict) and (src := img.get(\"src\")):\n",
    "                            images.append(src)\n",
    "            \n",
    "            # Combine content and create chunk\n",
    "            combined_content = \" \".join(str(p).strip() for p in content_parts if str(p).strip()).strip()\n",
    "            \n",
    "            if combined_content:\n",
    "                token_count = self.estimate_tokens(combined_content)\n",
    "                chunks.append(Chunk(\n",
    "                    component=component,\n",
    "                    section_title=section_title,\n",
    "                    section_path=section_path,\n",
    "                    content=combined_content,\n",
    "                    demo_files=demo_files,\n",
    "                    images=images,\n",
    "                    token_count=token_count\n",
    "                ))\n",
    "            \n",
    "            # Process children recursively\n",
    "            if children := section.get(\"children\"):\n",
    "                self._process_sections(component, children, chunks, section_path)\n",
    "            elif subsections := section.get(\"subsections\"):\n",
    "                self._process_sections(component, subsections, chunks, section_path)\n",
    "    \n",
    "    def chunk_documents(self, documents: List[dict]) -> List[Chunk]:\n",
    "        \"\"\"Chunk multiple documents and return all chunks.\"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            component = doc.get(\"component\") or doc.get(\"name\", \"unknown\")\n",
    "            sections = doc.get(\"sections\", [])\n",
    "            self._process_sections(component, sections, all_chunks)\n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "print(\"TextChunker class loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a203b9",
   "metadata": {},
   "source": [
    "## 2. Load Parsed Documents\n",
    "\n",
    "Loads all processed JSON files from `data/processed/` into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cc9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found JSON files: 71\n",
      "\n",
      "✓ 71 documents loaded\n",
      "  Directory: /home/sinan/GitHub/reservix/ai-assistant/etl/data/processed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / 'data/processed'\n",
    "CHUNKS_DIR = PROJECT_ROOT / 'data/chunks'\n",
    "\n",
    "# Load all processed JSON files\n",
    "json_files = sorted(PROCESSED_DATA_DIR.glob('*.json'))\n",
    "print(f\"Found JSON files: {len(json_files)}\\n\")\n",
    "\n",
    "docs = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        doc = json.load(f)\n",
    "        docs.append(doc)\n",
    "\n",
    "print(f\"{len(docs)} documents loaded\")\n",
    "print(f\"Directory: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd186d",
   "metadata": {},
   "source": [
    "## 3. Create Chunks with Parent-Child Relationships\n",
    "\n",
    "Processes all documents and creates semantic chunks with:\n",
    "- Unique chunk IDs\n",
    "- Parent ID for hierarchical relationships\n",
    "- Hierarchy levels (0=Root, 1=Subsection, etc.)\n",
    "- Token statistics for each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c03573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 606 chunks created\n",
      "\n",
      "Hierarchy levels:\n",
      "  Level 0:     70 chunks\n",
      "  Level 1:    282 chunks\n",
      "  Level 2:    254 chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize chunker and process all documents\n",
    "all_chunks = []\n",
    "chunk_id_counter = 1\n",
    "\n",
    "# Process each document with new flat structure\n",
    "for doc in docs:\n",
    "    component = doc.get(\"component\", \"unknown\")\n",
    "    sections = doc.get(\"sections\", [])\n",
    "    \n",
    "    # Build demo_map from top-level demos in the document\n",
    "    demo_map = {}\n",
    "    if top_level_demos := doc.get(\"demos\"):\n",
    "        for demo in top_level_demos:\n",
    "            if isinstance(demo, dict) and (demo_file := demo.get(\"file\")) and (code := demo.get(\"code\")):\n",
    "                demo_map[demo_file] = code\n",
    "    \n",
    "    # Track heading -> chunk_id mapping for parent_id resolution\n",
    "    heading_to_id = {}\n",
    "    \n",
    "    # Process flat sections list (new structure)\n",
    "    for section in sections:\n",
    "        heading = section.get(\"heading\", \"\")\n",
    "        level = section.get(\"level\", 0)\n",
    "        content = section.get(\"content\", \"\")\n",
    "        path = section.get(\"path\", [])\n",
    "        parent = section.get(\"parent\")\n",
    "        demo_files = section.get(\"demos\", [])\n",
    "        \n",
    "        # Skip empty sections\n",
    "        if not content.strip():\n",
    "            continue\n",
    "        \n",
    "        # Build section_path from path array\n",
    "        section_path = \" > \".join(path) if path else heading\n",
    "        \n",
    "        # Get demo code from demo_map\n",
    "        demo_code = {}\n",
    "        for demo_file in demo_files:\n",
    "            if demo_file in demo_map:\n",
    "                demo_code[demo_file] = demo_map[demo_file]\n",
    "        \n",
    "        # Resolve parent_id from parent heading name\n",
    "        parent_id = heading_to_id.get(parent) if parent else None\n",
    "        \n",
    "        chunk_dict = {\n",
    "            'id': chunk_id_counter,\n",
    "            'component': component,\n",
    "            'section_path': section_path,\n",
    "            'heading': heading,\n",
    "            'content': content,\n",
    "            'demo_code': demo_code,\n",
    "            'parent_id': parent_id,\n",
    "            'level': level\n",
    "        }\n",
    "        all_chunks.append(chunk_dict)\n",
    "        \n",
    "        # Map heading to chunk_id for parent resolution\n",
    "        heading_to_id[heading] = chunk_id_counter\n",
    "        chunk_id_counter += 1\n",
    "\n",
    "print(f\"{len(all_chunks)} chunks created\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85982691",
   "metadata": {},
   "source": [
    "## 4. Save Chunks to JSON\n",
    "\n",
    "Saves all chunks with metadata to `data/chunks/chunks.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1538ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved to: /home/sinan/GitHub/reservix/ai-assistant/etl/data/chunks/chunks.json\n",
      "File size: 0.45 MB\n",
      "Total chunks: 606\n"
     ]
    }
   ],
   "source": [
    "# Save all chunks to JSON file\n",
    "CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "output_file = CHUNKS_DIR / 'chunks.json'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "file_size_mb = output_file.stat().st_size / (1024 * 1024)\n",
    "print(f\"Chunks saved to: {output_file}\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
