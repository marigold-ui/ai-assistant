{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d20124",
   "metadata": {},
   "source": [
    "# Database Pipeline\n",
    "\n",
    "Load embeddings from JSON and store them in PostgreSQL with pgvector support.\n",
    "\n",
    "**Features:**\n",
    "- Load embeddings from chunks_primitive_embedded.json\n",
    "- Create tables with pgvector extension\n",
    "- Insert embeddings with vector indexing (IVF)\n",
    "- Query similar chunks by vector similarity\n",
    "- Database statistics and management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c48a24",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Before using this module, ensure PostgreSQL is configured:\n",
    "\n",
    "```sql\n",
    "-- Create pgvector extension\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\n",
    "-- Create database\n",
    "CREATE DATABASE marigold_rag;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1217d2",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed773478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatabaseConnector module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "class DatabaseConnector:\n",
    "    \"\"\"PostgreSQL database connector with pgvector support.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str = None,\n",
    "        port: int = None,\n",
    "        database: str = None,\n",
    "        user: str = None,\n",
    "        password: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize database connection parameters.\n",
    "        \n",
    "        Args:\n",
    "            host: Database host (defaults to DB_HOST env var or localhost)\n",
    "            port: Database port (defaults to DB_PORT env var or 5432)\n",
    "            database: Database name (defaults to DB_NAME env var or marigold_rag)\n",
    "            user: Database user (defaults to DB_USER env var or postgres)\n",
    "            password: Database password (defaults to DB_PASSWORD env var)\n",
    "        \"\"\"\n",
    "        self.host = host or os.getenv(\"DB_HOST\", \"localhost\")\n",
    "        self.port = int(port or os.getenv(\"DB_PORT\", 5432))\n",
    "        self.database = database or os.getenv(\"DB_NAME\", \"marigold_rag\")\n",
    "        self.user = user or os.getenv(\"DB_USER\", \"postgres\")\n",
    "        self.password = password or os.getenv(\"DB_PASSWORD\")\n",
    "        \n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        try:\n",
    "            import psycopg2\n",
    "            from psycopg2 import pool\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"psycopg2 package required. Install with: pip install psycopg2-binary\"\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                database=self.database,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            self.cursor = self.connection.cursor()\n",
    "            print(f\"Connected to {self.database} on {self.host}:{self.port}\")\n",
    "        except psycopg2.OperationalError as e:\n",
    "            raise ConnectionError(f\"Failed to connect to database: {e}\")\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"Disconnected from database\")\n",
    "    \n",
    "    def _execute(self, query: str, params: tuple = None):\n",
    "        \"\"\"Execute a query.\"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(query, params or ())\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            self.connection.rollback()\n",
    "            raise RuntimeError(f\"Query execution failed: {e}\\nQuery: {query}\")\n",
    "    \n",
    "    def _fetch_all(self, query: str, params: tuple = None):\n",
    "        \"\"\"Execute a query and fetch all results.\"\"\"\n",
    "        self.cursor.execute(query, params or ())\n",
    "        return self.cursor.fetchall()\n",
    "    \n",
    "    def create_tables(self):\n",
    "        \"\"\"Create necessary tables if they don't exist.\"\"\"\n",
    "        try:\n",
    "            import psycopg2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"psycopg2 package required\")\n",
    "        \n",
    "        # Enable pgvector extension\n",
    "        try:\n",
    "            self._execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "        except:\n",
    "            print(\"Warning: Could not create vector extension (may already exist)\")\n",
    "        \n",
    "        # Create chunks table\n",
    "        create_chunks_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS chunks (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            component VARCHAR(255) NOT NULL,\n",
    "            section_title VARCHAR(500),\n",
    "            section_path TEXT,\n",
    "            content TEXT NOT NULL,\n",
    "            demo_files TEXT,\n",
    "            images TEXT,\n",
    "            token_count INTEGER,\n",
    "            embedding vector(768),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            CONSTRAINT valid_content CHECK (char_length(content) > 0)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create index for vector similarity search\n",
    "        create_index = \"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS chunks_embedding_idx \n",
    "        ON chunks USING ivfflat (embedding vector_cosine_ops)\n",
    "        WITH (lists = 100);\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create metadata table for tracking\n",
    "        create_metadata_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS processing_metadata (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            component VARCHAR(255),\n",
    "            total_chunks INTEGER,\n",
    "            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        self._execute(create_chunks_table)\n",
    "        self._execute(create_index)\n",
    "        self._execute(create_metadata_table)\n",
    "        \n",
    "        print(\"Database tables created successfully\")\n",
    "    \n",
    "    def store_chunk(\n",
    "        self,\n",
    "        component: str,\n",
    "        section_title: Optional[str],\n",
    "        section_path: Optional[str],\n",
    "        content: str,\n",
    "        embedding: List[float],\n",
    "        demo_files: List[str] = None,\n",
    "        images: List[str] = None,\n",
    "        token_count: int = 0\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Store a chunk with its embedding.\n",
    "        \n",
    "        Args:\n",
    "            component: Component name\n",
    "            section_title: Section title\n",
    "            section_path: Section path\n",
    "            content: Chunk content\n",
    "            embedding: Embedding vector\n",
    "            demo_files: List of demo files\n",
    "            images: List of image paths\n",
    "            token_count: Token count for chunk\n",
    "        \n",
    "        Returns:\n",
    "            Chunk ID\n",
    "        \"\"\"\n",
    "        # Convert lists to JSON strings\n",
    "        demo_files_json = json.dumps(demo_files or [])\n",
    "        images_json = json.dumps(images or [])\n",
    "        \n",
    "        # Convert embedding to string format for pgvector\n",
    "        embedding_str = \"[\" + \",\".join(str(x) for x in embedding) + \"]\"\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO chunks \n",
    "        (component, section_title, section_path, content, embedding, \n",
    "         demo_files, images, token_count)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        RETURNING id;\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cursor.execute(\n",
    "            insert_query,\n",
    "            (component, section_title, section_path, content, embedding_str,\n",
    "             demo_files_json, images_json, token_count)\n",
    "        )\n",
    "        self.connection.commit()\n",
    "        \n",
    "        chunk_id = self.cursor.fetchone()[0]\n",
    "        return chunk_id\n",
    "    \n",
    "    def search_similar(\n",
    "        self,\n",
    "        embedding: List[float],\n",
    "        limit: int = 5,\n",
    "        threshold: float = 0.7\n",
    "    ) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Search for similar chunks using vector similarity.\n",
    "        \n",
    "        Args:\n",
    "            embedding: Query embedding vector\n",
    "            limit: Maximum number of results\n",
    "            threshold: Cosine similarity threshold (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            List of (chunk_id, component, section_title, content, similarity_score)\n",
    "        \"\"\"\n",
    "        embedding_str = \"[\" + \",\".join(str(x) for x in embedding) + \"]\"\n",
    "        \n",
    "        search_query = \"\"\"\n",
    "        SELECT id, component, section_title, content,\n",
    "               1 - (embedding <=> %s::vector) as similarity\n",
    "        FROM chunks\n",
    "        WHERE 1 - (embedding <=> %s::vector) > %s\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "        \"\"\"\n",
    "        \n",
    "        results = self._fetch_all(\n",
    "            search_query,\n",
    "            (embedding_str, embedding_str, threshold, embedding_str, limit)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_chunk(self, chunk_id: int) -> Optional[Tuple]:\n",
    "        \"\"\"Get a chunk by ID.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT id, component, section_title, content, demo_files, images, \n",
    "               token_count, created_at\n",
    "        FROM chunks\n",
    "        WHERE id = %s;\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self._fetch_all(query, (chunk_id,))\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def get_chunks_by_component(self, component: str) -> List[Tuple]:\n",
    "        \"\"\"Get all chunks for a component.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT id, component, section_title, content, token_count\n",
    "        FROM chunks\n",
    "        WHERE component = %s\n",
    "        ORDER BY created_at;\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._fetch_all(query, (component,))\n",
    "    \n",
    "    def delete_component_chunks(self, component: str) -> int:\n",
    "        \"\"\"Delete all chunks for a component (for reprocessing).\"\"\"\n",
    "        query = \"DELETE FROM chunks WHERE component = %s;\"\n",
    "        \n",
    "        self.cursor.execute(query, (component,))\n",
    "        self.connection.commit()\n",
    "        \n",
    "        return self.cursor.rowcount\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get database statistics.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # Total chunks\n",
    "        result = self._fetch_all(\"SELECT COUNT(*) FROM chunks;\")\n",
    "        stats[\"total_chunks\"] = result[0][0] if result else 0\n",
    "        \n",
    "        # Chunks per component\n",
    "        result = self._fetch_all(\n",
    "            \"SELECT component, COUNT(*) as count FROM chunks GROUP BY component;\"\n",
    "        )\n",
    "        stats[\"chunks_by_component\"] = {row[0]: row[1] for row in result}\n",
    "        \n",
    "        # Average token count\n",
    "        result = self._fetch_all(\"SELECT AVG(token_count) FROM chunks;\")\n",
    "        stats[\"avg_token_count\"] = float(result[0][0]) if result and result[0][0] else 0\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "print(\"DatabaseConnector module loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a09db",
   "metadata": {},
   "source": [
    "## Section 2: Load and Store Embeddings\n",
    "\n",
    "Load embeddings from JSON file and insert into database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d57e123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: /home/sinan/GitHub/reservix/ai-assistant/etl/data/chunks/chunks_primitive_embedded.json\n",
      "File exists: True\n",
      "Loaded 186 chunks with embeddings\n",
      "Embedding dimensions: 768\n",
      "Sample keys: ['component', 'section_title', 'section_path', 'content', 'demo_files', 'images', 'token_count', 'embedding']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load embeddings from file\n",
    "CHUNKS_DIR = Path('/home/sinan/GitHub/reservix/ai-assistant/etl/data/chunks')\n",
    "embeddings_file = CHUNKS_DIR / 'chunks_primitive_embedded.json'\n",
    "\n",
    "print(f\"Loading embeddings from: {embeddings_file}\")\n",
    "print(f\"File exists: {embeddings_file.exists()}\")\n",
    "\n",
    "try:\n",
    "    with open(embeddings_file) as f:\n",
    "        chunks_with_embeddings = json.load(f)\n",
    "    print(f\"Loaded {len(chunks_with_embeddings)} chunks with embeddings\")\n",
    "    \n",
    "    # Check embedding dimensions\n",
    "    first_embedding = chunks_with_embeddings[0].get('embedding', [])\n",
    "    print(f\"Embedding dimensions: {len(first_embedding)}\")\n",
    "    print(f\"Sample keys: {list(chunks_with_embeddings[0].keys())}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Run the embedder notebook first to generate embeddings\")\n",
    "    chunks_with_embeddings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4107c",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "Configure database connection via environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96951fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to marigold_rag on localhost:5432\n",
      "Database tables created successfully\n",
      "Tables created successfully\n",
      "\n",
      "Storing 186 chunks...\n",
      "  Stored 50/186 chunks...\n",
      "  Stored 100/186 chunks...\n",
      "  Stored 150/186 chunks...\n",
      "\n",
      "Successfully stored 186/186 chunks\n",
      "\n",
      "Database Statistics:\n",
      "  Total chunks: 186\n",
      "  Chunks by component: {'tiles': 2, 'inset': 2, 'cva': 1, 'cn': 1, 'breadcrumbs': 2, 'inline': 3, 'button': 7, 'stack': 3, 'textarea': 2, 'multiselect': 4, 'slider': 4, 'selectlist': 2, 'link-button': 2, 'svg': 2, 'switch': 2, 'center': 1, 'number-field': 6, 'dialog': 4, 'toast': 2, 'useListData': 1, 'drawer': 3, 'file-field': 1, 'accordion': 2, 'aside': 2, 'section-message': 3, 'columns': 3, 'useAsyncListData': 1, 'calendar': 2, 'aspect': 2, 'scrollable': 2, 'useTheme': 1, 'numericformat': 3, 'timefield': 2, 'visually-hidden': 2, 'dateformat': 3, 'checkbox': 6, 'text': 2, 'overview': 4, 'pagination': 5, 'grid': 2, 'tooltip': 2, 'datepicker': 3, 'select': 4, 'provider': 2, 'textfield': 4, 'divider': 1, 'datefield': 3, 'parseFormData': 2, 'split': 2, 'routerprovider': 1, 'table': 9, 'loader': 2, 'contextual-help': 2, 'container': 3, 'menu': 3, 'breakout': 2, 'headline': 2, 'tabs': 2, 'list': 2, 'radio': 4, 'icon': 3, 'card': 2, 'extendTheme': 1, 'form': 2, 'useResponsiveValue': 1, 'search-field': 2, 'autocomplete': 4, 'badge': 2, 'link': 4, 'tag': 4, 'combobox': 2}\n",
      "  Avg token count: 404.2\n",
      "Disconnected from database\n"
     ]
    }
   ],
   "source": [
    "from psycopg2 import Error\n",
    "\n",
    "# Initialize database connector\n",
    "db = DatabaseConnector(\n",
    "    host=os.getenv(\"DB_HOST\", \"localhost\"),\n",
    "    port=int(os.getenv(\"DB_PORT\", 5432)),\n",
    "    database=os.getenv(\"DB_NAME\", \"marigold_rag\"),\n",
    "    user=os.getenv(\"DB_USER\", \"postgres\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\", \"postgres\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Connect and create tables\n",
    "    db.connect()\n",
    "    db.create_tables()\n",
    "    print(\"Tables created successfully\\n\")\n",
    "    \n",
    "    # Store all chunks with embeddings\n",
    "    if chunks_with_embeddings:\n",
    "        print(f\"Storing {len(chunks_with_embeddings)} chunks...\")\n",
    "        \n",
    "        stored_count = 0\n",
    "        for i, chunk in enumerate(chunks_with_embeddings):\n",
    "            try:\n",
    "                chunk_id = db.store_chunk(\n",
    "                    component=chunk.get('component', 'unknown'),\n",
    "                    section_title=chunk.get('section_title'),\n",
    "                    section_path=chunk.get('section_path'),\n",
    "                    content=chunk.get('content', ''),\n",
    "                    embedding=chunk.get('embedding', []),\n",
    "                    demo_files=chunk.get('demo_files', []),\n",
    "                    images=chunk.get('images', []),\n",
    "                    token_count=chunk.get('token_count', 0)\n",
    "                )\n",
    "                stored_count += 1\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"  Stored {i + 1}/{len(chunks_with_embeddings)} chunks...\")\n",
    "            \n",
    "            except Error as e:\n",
    "                print(f\"Error storing chunk {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nSuccessfully stored {stored_count}/{len(chunks_with_embeddings)} chunks\")\n",
    "        \n",
    "        # Show statistics\n",
    "        stats = db.get_stats()\n",
    "        print(f\"\\nDatabase Statistics:\")\n",
    "        print(f\"  Total chunks: {stats['total_chunks']}\")\n",
    "        print(f\"  Chunks by component: {stats['chunks_by_component']}\")\n",
    "        print(f\"  Avg token count: {stats['avg_token_count']:.1f}\")\n",
    "    else:\n",
    "        print(\"No embeddings loaded - skipping insert\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    db.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
